{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMsrUYmVcdBK+Sl4Vw/yxnA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Build Recurrent Neural Network (RNN)"],"metadata":{"id":"_lIiczCSadHC"}},{"cell_type":"code","source":["# Import necessary libraries\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, SimpleRNN, Embedding\n","# Import the Tokenizer class from the text module in TensorFlow Keras preprocessing\n","# This class is used to vectorize a text corpus, by turning each text into either a sequence of integers\n","# (each integer being the index of a token in a dictionary) or into a vector where the coefficient(s)\n","# for each token could be binary, based on word count, based on tf-idf...\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","# Import the pad_sequences function from the sequence module in TensorFlow Keras preprocessing\n","# This function is used to ensure that all sequences in a list have the same length,\n","# by padding shorter sequences with a specified value (by default 0) or truncating longer sequences\n","# to a specified length.\n","from tensorflow.keras.preprocessing.sequence import pad_sequences"],"metadata":{"id":"kwkrUCnzapPM","executionInfo":{"status":"ok","timestamp":1724366160469,"user_tz":-540,"elapsed":4313,"user":{"displayName":"Rao Hammad Raza","userId":"12244803790304730718"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["# Sample Data for Training"],"metadata":{"id":"PEV5p6OpbYH5"}},{"cell_type":"code","source":["\n","# Sample data\n","sentences = [\n","    \"I love pakistan\"\n","    \"Machine learning algorithms are powerful tools.\"\n","    \"I enjoy exploring new algorithms.\"\n","    \"Learning about AI is captivating.\"\n","    \"Deep learning models can be complex.\"\n","    \"I love understanding how neural networks work.\"\n","    \"The field of data science is evolving rapidly.\"\n","    \"I find artificial intelligence intriguing.\"\n","    \"Studying computer vision is exciting.\"\n","    \"Natural language processing is a fascinating domain.\"\n","    \"I enjoy coding in Python for data science projects.\"\n","    \"Building models is a creative process.\"\n","    \"I love experimenting with different machine learning techniques.\"\n","    \"The potential of AI to transform industries is amazing.\"\n","    \"I enjoy staying updated with the latest tech trends.\"\n","    \"Learning about reinforcement learning is interesting.\"\n","    \"I find predictive modeling to be very useful.\"\n","    \"I love solving problems with data analysis.\"\n","    \"Data preprocessing is a crucial step in machine learning.\"\n","    \"I enjoy reading research papers on deep learning.\"\n","    \"I find optimization techniques fascinating.\"\n","    \"Understanding algorithms helps in developing better solutions.\"\n","    \"I love the challenge of debugging code.\"\n","    \"Machine learning applications are diverse and impactful.\"\n","    \"I enjoy collaborating with others on tech projects.\"\n","    \"Learning new programming languages is fun.\"\n","    \"I love working with large datasets.\"\n","    \"I find feature engineering to be an art.\"\n","    \"Model evaluation is an essential part of machine learning.\"\n","    \"I enjoy attending tech conferences.\"\n","    \"Learning about big data technologies is exciting.\"\n","    \"I love experimenting with neural network architectures.\"\n","    \"I find the theory behind machine learning algorithms interesting.\"\n","    \"I enjoy visualizing data insights.\"\n","    \"Machine learning models can make accurate predictions.\"\n","    \"I love the creativity involved in data storytelling.\"\n","    \"I find unsupervised learning techniques intriguing.\"\n","    \"I enjoy automating tasks with AI.\"\n","    \"Learning about AI ethics is important.\"\n","    \"I love the problem-solving aspect of machine learning.\"\n","    \"I find cloud computing technologies fascinating.\"\n","    \"I enjoy using machine learning for real-world applications.\"\n","    \"I love experimenting with different data preprocessing techniques.\"\n","    \"I find transfer learning to be a powerful approach.\"\n","    \"I enjoy working on machine learning projects.\"\n","    \"Learning about data privacy is crucial.\"\n","    \"I love the innovation happening in the AI field.\"\n","    \"I find data visualization tools useful.\"\n","    \"I enjoy testing and validating machine learning models.\"\n","    \"I love discovering new machine learning applications.\"\n","    \"I find ensemble methods to be effective.\"\n","    \"I enjoy learning from data.\"\n","    \"Machine learning can provide valuable insights.\"\n","    \"I love the interdisciplinary nature of AI.\"\n","    \"I find recommendation systems interesting.\"\n","    \"I enjoy participating in hackathons.\"\n","    \"Learning about neural networks is fascinating.\"\n","    \"I love the potential of AI to solve complex problems.\"\n","    \"I find sentiment analysis intriguing.\"\n","    \"I enjoy implementing machine learning algorithms.\"\n","    \"I love the excitement of discovering patterns in data.\"\n","    \"I find time series analysis challenging.\"\n","    \"I enjoy exploring different types of data.\"\n","    \"Machine learning is transforming various industries.\"\n","    \"I love working on predictive analytics.\"\n","    \"I find anomaly detection to be useful.\"\n","    \"I enjoy studying the mathematics behind machine learning.\"\n","    \"I love the hands-on experience of building models.\"\n","    \"I find clustering techniques interesting.\"\n","    \"I enjoy exploring open-source machine learning libraries.\"\n","    \"Machine learning can automate complex tasks.\"\n","    \"I love the flexibility of machine learning models.\"\n","    \"I find computer vision applications fascinating.\"\n","    \"I enjoy solving real-world problems with AI.\"\n","    \"I love the continuous learning aspect of AI.\"\n","    \"I find reinforcement learning to be challenging.\"\n","    \"I enjoy experimenting with hyperparameter tuning.\"\n","    \"Machine learning can improve decision-making processes.\"\n","    \"I love the creativity involved in feature selection.\"\n","    \"I find generative models to be fascinating.\"\n","    \"I enjoy reading about the latest AI advancements.\"\n","    \"Machine learning can enhance user experiences.\"\n","    \"I love the diversity of machine learning applications.\"\n","    \"I find natural language generation intriguing.\"\n","    \"I enjoy working with text data.\"\n","    \"Machine learning can optimize business processes.\"\n","    \"I love the innovation in AI research.\"\n","    \"I find the concept of machine learning interpretability interesting.\"\n","    \"I enjoy creating machine learning workflows.\"\n","    \"Machine learning can uncover hidden patterns.\"\n","    \"I love the impact of AI on society.\"\n","    \"I find deep reinforcement learning fascinating.\"\n","    \"I enjoy developing custom machine learning solutions.\"\n","    \"Machine learning can improve customer experiences.\"\n","    \"I love the potential of AI in healthcare.\"\n","    \"I find the scalability of machine learning models intriguing.\"\n","    \"I enjoy applying machine learning to finance.\"\n","    \"Machine learning can enhance security measures.\"\n","    \"I love the possibilities of AI in creative industries.\"\n","    \"I find the ethical implications of AI important.\"\n","    \"I enjoy sharing knowledge about machine learning.\"\n","    \"This Free Advance AI Course that is helping alot of students to learn the concepts of AI and provides the detailed guideline on how to learn AI. This course enables the students to make their own projects. Updates them with the state of the art technologies and provide all the necessary knowlegde so that they should not be dependend on anyone to be able to learn anything.\"\n","]"],"metadata":{"id":"hAa_RkDBbWzw","executionInfo":{"status":"ok","timestamp":1724366162880,"user_tz":-540,"elapsed":410,"user":{"displayName":"Rao Hammad Raza","userId":"12244803790304730718"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["# Tokenization and Preprocessing of Data"],"metadata":{"id":"uktPEAcBbxqf"}},{"cell_type":"code","source":["# Initialize the tokenizer\n","tokenizer = Tokenizer()\n","\n","# Fit the tokenizer on the provided sentences\n","tokenizer.fit_on_texts(sentences)\n","\n","# Get the total number of unique words (plus one for padding)\n","total_words = len(tokenizer.word_index) + 1\n","print(total_words)\n","print(tokenizer.word_index)\n","\n","# Initialize a list to hold input sequences\n","input_sequences = []\n","\n","# Iterate over each sentence\n","for line in sentences:\n","    # Convert the sentence to a sequence of integers\n","    token_list = tokenizer.texts_to_sequences([line])[0]\n","    # print(token_list)\n","    # Create n-gram sequences\n","    for i in range(1, len(token_list)):\n","        n_gram_sequence = token_list[:i+1]\n","        input_sequences.append(n_gram_sequence)\n","\n","\n","\n","\n","# Determine the maximum sequence length\n","max_sequence_len = max([len(x) for x in input_sequences])\n","\n","# Pad sequences to ensure they are all the same length\n","input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n","print(input_sequences)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Eq3PYoS-b4ny","executionInfo":{"status":"ok","timestamp":1724366168405,"user_tz":-540,"elapsed":402,"user":{"displayName":"Rao Hammad Raza","userId":"12244803790304730718"}},"outputId":"57f56f4c-6072-4ed7-a86b-b955be0c8632"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["255\n","{'i': 1, 'learning': 2, 'the': 3, 'machine': 4, 'love': 5, 'enjoy': 6, 'find': 7, 'of': 8, 'ai': 9, 'is': 10, 'to': 11, 'data': 12, 'with': 13, 'in': 14, 'can': 15, 'be': 16, 'about': 17, 'models': 18, 'on': 19, 'fascinating': 20, 'algorithms': 21, 'intriguing': 22, 'techniques': 23, 'interesting': 24, 'applications': 25, 'a': 26, 'projects': 27, 'experimenting': 28, 'and': 29, 'working': 30, 'exploring': 31, 'new': 32, 'deep': 33, 'complex': 34, 'neural': 35, 'different': 36, 'potential': 37, 'industries': 38, 'tech': 39, 'reinforcement': 40, 'useful': 41, 'solving': 42, 'problems': 43, 'analysis': 44, 'technologies': 45, 'learn': 46, 'are': 47, 'powerful': 48, 'tools': 49, 'understanding': 50, 'how': 51, 'networks': 52, 'field': 53, 'science': 54, 'studying': 55, 'computer': 56, 'vision': 57, 'exciting': 58, 'natural': 59, 'language': 60, 'for': 61, 'building': 62, 'creative': 63, 'latest': 64, 'predictive': 65, 'preprocessing': 66, 'crucial': 67, 'reading': 68, 'research': 69, 'developing': 70, 'solutions': 71, 'feature': 72, 'an': 73, 'art': 74, 'behind': 75, 'insights': 76, 'make': 77, 'creativity': 78, 'involved': 79, 'tasks': 80, 'important': 81, 'aspect': 82, 'real': 83, 'world': 84, 'innovation': 85, 'discovering': 86, 'provide': 87, 'patterns': 88, 'challenging': 89, 'improve': 90, 'processes': 91, 'enhance': 92, 'experiences': 93, 'this': 94, 'course': 95, 'that': 96, 'students': 97, 'pakistanmachine': 98, 'captivating': 99, 'work': 100, 'evolving': 101, 'rapidly': 102, 'artificial': 103, 'intelligence': 104, 'processing': 105, 'domain': 106, 'coding': 107, 'python': 108, 'process': 109, 'transform': 110, 'amazing': 111, 'staying': 112, 'updated': 113, 'trends': 114, 'modeling': 115, 'very': 116, 'step': 117, 'papers': 118, 'optimization': 119, 'helps': 120, 'better': 121, 'challenge': 122, 'debugging': 123, 'code': 124, 'diverse': 125, 'impactful': 126, 'collaborating': 127, 'others': 128, 'programming': 129, 'languages': 130, 'fun': 131, 'large': 132, 'datasets': 133, 'engineering': 134, 'model': 135, 'evaluation': 136, 'essential': 137, 'part': 138, 'attending': 139, 'conferences': 140, 'big': 141, 'network': 142, 'architectures': 143, 'theory': 144, 'visualizing': 145, 'accurate': 146, 'predictions': 147, 'storytelling': 148, 'unsupervised': 149, 'automating': 150, 'ethics': 151, 'problem': 152, 'cloud': 153, 'computing': 154, 'using': 155, 'transfer': 156, 'approach': 157, 'privacy': 158, 'happening': 159, 'visualization': 160, 'testing': 161, 'validating': 162, 'ensemble': 163, 'methods': 164, 'effective': 165, 'from': 166, 'valuable': 167, 'interdisciplinary': 168, 'nature': 169, 'recommendation': 170, 'systems': 171, 'participating': 172, 'hackathons': 173, 'solve': 174, 'sentiment': 175, 'implementing': 176, 'excitement': 177, 'time': 178, 'series': 179, 'types': 180, 'transforming': 181, 'various': 182, 'analytics': 183, 'anomaly': 184, 'detection': 185, 'mathematics': 186, 'hands': 187, 'experience': 188, 'clustering': 189, 'open': 190, 'source': 191, 'libraries': 192, 'automate': 193, 'flexibility': 194, 'continuous': 195, 'hyperparameter': 196, 'tuning': 197, 'decision': 198, 'making': 199, 'selection': 200, 'generative': 201, 'advancements': 202, 'user': 203, 'diversity': 204, 'generation': 205, 'text': 206, 'optimize': 207, 'business': 208, 'concept': 209, 'interpretability': 210, 'creating': 211, 'workflows': 212, 'uncover': 213, 'hidden': 214, 'impact': 215, 'society': 216, 'custom': 217, 'customer': 218, 'healthcare': 219, 'scalability': 220, 'applying': 221, 'finance': 222, 'security': 223, 'measures': 224, 'possibilities': 225, 'ethical': 226, 'implications': 227, 'sharing': 228, 'knowledge': 229, 'free': 230, 'advance': 231, 'helping': 232, 'alot': 233, 'concepts': 234, 'provides': 235, 'detailed': 236, 'guideline': 237, 'enables': 238, 'their': 239, 'own': 240, 'updates': 241, 'them': 242, 'state': 243, 'all': 244, 'necessary': 245, 'knowlegde': 246, 'so': 247, 'they': 248, 'should': 249, 'not': 250, 'dependend': 251, 'anyone': 252, 'able': 253, 'anything': 254}\n","[[  0   0   0 ...   0   1   5]\n"," [  0   0   0 ...   1   5  98]\n"," [  0   0   0 ...   5  98   2]\n"," ...\n"," [  0   0   1 ...  16 253  11]\n"," [  0   1   5 ... 253  11  46]\n"," [  1   5  98 ...  11  46 254]]\n"]}]},{"cell_type":"markdown","source":["# Data Preparation:"],"metadata":{"id":"B9S3uB2vgWIs"}},{"cell_type":"code","source":["# Split data into inputs and labels\n","\n","# Inputs: All elements of the sequences except the last one\n","X = input_sequences[:, :-1]\n","print(\"Input Data: \", X)\n","\n","# Labels : The last element of each sequence (the word to predict)\n","y = input_sequences[:, -1]\n","print(\"Labels: \", y)\n","\n","# Convert labels to one-hot encoded format\n","y = tf.keras.utils.to_categorical(y, num_classes=total_words)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q2iA-YqDgaNE","executionInfo":{"status":"ok","timestamp":1724366172860,"user_tz":-540,"elapsed":439,"user":{"displayName":"Rao Hammad Raza","userId":"12244803790304730718"}},"outputId":"2b48cfcd-c20a-486d-d565-5452617cede0"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Input Data:  [[  0   0   0 ...   0   0   1]\n"," [  0   0   0 ...   0   1   5]\n"," [  0   0   0 ...   1   5  98]\n"," ...\n"," [  0   0   1 ...  11  16 253]\n"," [  0   1   5 ...  16 253  11]\n"," [  1   5  98 ... 253  11  46]]\n","Labels:  [  5  98   2  21  47  48  49   1   6  31  32  21   2  17   9  10  99  33\n","   2  18  15  16  34   1   5  50  51  35  52 100   3  53   8  12  54  10\n"," 101 102   1   7 103 104  22  55  56  57  10  58  59  60 105  10  26  20\n"," 106   1   6 107  14 108  61  12  54  27  62  18  10  26  63 109   1   5\n","  28  13  36   4   2  23   3  37   8   9  11 110  38  10 111   1   6 112\n"," 113  13   3  64  39 114   2  17  40   2  10  24   1   7  65 115  11  16\n"," 116  41   1   5  42  43  13  12  44  12  66  10  26  67 117  14   4   2\n","   1   6  68  69 118  19  33   2   1   7 119  23  20  50  21 120  14  70\n"," 121  71   1   5   3 122   8 123 124   4   2  25  47 125  29 126   1   6\n"," 127  13 128  19  39  27   2  32 129 130  10 131   1   5  30  13 132 133\n","   1   7  72 134  11  16  73  74 135 136  10  73 137 138   8   4   2   1\n","   6 139  39 140   2  17 141  12  45  10  58   1   5  28  13  35 142 143\n","   1   7   3 144  75   4   2  21  24   1   6 145  12  76   4   2  18  15\n","  77 146 147   1   5   3  78  79  14  12 148   1   7 149   2  23  22   1\n","   6 150  80  13   9   2  17   9 151  10  81   1   5   3 152  42  82   8\n","   4   2   1   7 153 154  45  20   1   6 155   4   2  61  83  84  25   1\n","   5  28  13  36  12  66  23   1   7 156   2  11  16  26  48 157   1   6\n","  30  19   4   2  27   2  17  12 158  10  67   1   5   3  85 159  14   3\n","   9  53   1   7  12 160  49  41   1   6 161  29 162   4   2  18   1   5\n","  86  32   4   2  25   1   7 163 164  11  16 165   1   6   2 166  12   4\n","   2  15  87 167  76   1   5   3 168 169   8   9   1   7 170 171  24   1\n","   6 172  14 173   2  17  35  52  10  20   1   5   3  37   8   9  11 174\n","  34  43   1   7 175  44  22   1   6 176   4   2  21   1   5   3 177   8\n","  86  88  14  12   1   7 178 179  44  89   1   6  31  36 180   8  12   4\n","   2  10 181 182  38   1   5  30  19  65 183   1   7 184 185  11  16  41\n","   1   6  55   3 186  75   4   2   1   5   3 187  19 188   8  62  18   1\n","   7 189  23  24   1   6  31 190 191   4   2 192   4   2  15 193  34  80\n","   1   5   3 194   8   4   2  18   1   7  56  57  25  20   1   6  42  83\n","  84  43  13   9   1   5   3 195   2  82   8   9   1   7  40   2  11  16\n","  89   1   6  28  13 196 197   4   2  15  90 198 199  91   1   5   3  78\n","  79  14  72 200   1   7 201  18  11  16  20   1   6  68  17   3  64   9\n"," 202   4   2  15  92 203  93   1   5   3 204   8   4   2  25   1   7  59\n","  60 205  22   1   6  30  13 206  12   4   2  15 207 208  91   1   5   3\n","  85  14   9  69   1   7   3 209   8   4   2 210  24   1   6 211   4   2\n"," 212   4   2  15 213 214  88   1   5   3 215   8   9  19 216   1   7  33\n","  40   2  20   1   6  70 217   4   2  71   4   2  15  90 218  93   1   5\n","   3  37   8   9  14 219   1   7   3 220   8   4   2  18  22   1   6 221\n","   4   2  11 222   4   2  15  92 223 224   1   5   3 225   8   9  14  63\n","  38   1   7   3 226 227   8   9  81   1   6 228 229  17   4   2  94 230\n"," 231   9  95  96  10 232 233   8  97  11  46   3 234   8   9  29 235   3\n"," 236 237  19  51  11  46   9  94  95 238   3  97  11  77 239 240  27 241\n"," 242  13   3 243   8   3  74  45  29  87 244   3 245 246 247  96 248 249\n"," 250  16 251  19 252  11  16 253  11  46 254]\n"]}]},{"cell_type":"markdown","source":["# Defining the Model Architecture:"],"metadata":{"id":"56Rpiufxhe7d"}},{"cell_type":"code","source":["# Define the RNN model\n","\n","# Sequential model allows stacking layers in a linear fashion\n","model = Sequential([\n","    # Embedding layer to convert word indices to dense vectors of fixed size\n","    # Input dimension: total number of words, Output dimension: size of embedding vectors\n","    # Input length: length of input sequences (excluding the last word)\n","    Embedding(total_words, 10, input_length=max_sequence_len-1),\n","\n","    # SimpleRNN layer with 50 units, which processes the sequence data\n","    SimpleRNN(50),\n","\n","    # Dense output layer with a softmax activation function\n","    # Output dimension: total number of words (for multi-class classification)\n","    Dense(total_words, activation='softmax')\n","])\n"],"metadata":{"id":"hL3VVA1qhlmr","executionInfo":{"status":"ok","timestamp":1724366177026,"user_tz":-540,"elapsed":13,"user":{"displayName":"Rao Hammad Raza","userId":"12244803790304730718"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"d6a691de-356f-405a-8d32-6af9845d0236"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["# Compile the modal\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"],"metadata":{"id":"ti28V8Shi2TL","executionInfo":{"status":"ok","timestamp":1724299983715,"user_tz":-540,"elapsed":713,"user":{"displayName":"Rao Hammad Raza","userId":"12244803790304730718"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# Train the model\n","model.fit(X, y, epochs=50, verbose=1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JgEzZdSEjOg9","executionInfo":{"status":"ok","timestamp":1724300379362,"user_tz":-540,"elapsed":393634,"user":{"displayName":"Rao Hammad Raza","userId":"12244803790304730718"}},"outputId":"8dfc19b5-7e99-453a-d944-c9aa043007c1"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 181ms/step - accuracy: 0.0038 - loss: 5.5341\n","Epoch 2/50\n","\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 236ms/step - accuracy: 0.0265 - loss: 5.3446\n","Epoch 3/50\n","\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 190ms/step - accuracy: 0.0386 - loss: 4.9555\n","Epoch 4/50\n","\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 259ms/step - accuracy: 0.0940 - loss: 4.6846\n","Epoch 5/50\n","\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 178ms/step - accuracy: 0.0880 - loss: 4.6849\n","Epoch 6/50\n","\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 278ms/step - accuracy: 0.0948 - loss: 4.5491\n","Epoch 7/50\n","\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 272ms/step - accuracy: 0.0861 - loss: 4.6128\n","Epoch 8/50\n","\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 284ms/step - accuracy: 0.0961 - loss: 4.6428\n","Epoch 9/50\n","\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 179ms/step - accuracy: 0.0981 - loss: 4.5833\n","Epoch 10/50\n","\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 280ms/step - accuracy: 0.0871 - loss: 4.6280\n","Epoch 11/50\n","\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 206ms/step - accuracy: 0.0871 - loss: 4.5653\n","Epoch 12/50\n","\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 312ms/step - accuracy: 0.0816 - loss: 4.5689\n","Epoch 13/50\n","\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 312ms/step - accuracy: 0.0961 - loss: 4.5754\n","Epoch 14/50\n","\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 219ms/step - accuracy: 0.1367 - loss: 4.4554\n","Epoch 15/50\n","\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 272ms/step - accuracy: 0.1825 - loss: 4.3625\n","Epoch 16/50\n","\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 271ms/step - accuracy: 0.1683 - loss: 4.3353\n","Epoch 17/50\n","\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 204ms/step - accuracy: 0.1882 - loss: 4.1980\n","Epoch 18/50\n","\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 305ms/step - accuracy: 0.2510 - loss: 3.9145\n","Epoch 19/50\n","\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 196ms/step - accuracy: 0.2518 - loss: 3.8656\n","Epoch 20/50\n","\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 260ms/step - accuracy: 0.2338 - loss: 3.7847\n","Epoch 21/50\n","\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 182ms/step - accuracy: 0.2874 - loss: 3.5548\n","Epoch 22/50\n","\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 203ms/step - accuracy: 0.2673 - loss: 3.6303\n","Epoch 23/50\n","\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 262ms/step - accuracy: 0.3174 - loss: 3.4240\n","Epoch 24/50\n","\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 324ms/step - accuracy: 0.2822 - loss: 3.4580\n","Epoch 25/50\n","\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 251ms/step - accuracy: 0.3023 - loss: 3.3981\n","Epoch 26/50\n","\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 248ms/step - accuracy: 0.3204 - loss: 3.2970\n","Epoch 27/50\n","\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 238ms/step - accuracy: 0.3156 - loss: 3.2946\n","Epoch 28/50\n","\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 215ms/step - accuracy: 0.3321 - loss: 3.1847\n","Epoch 29/50\n","\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 182ms/step - accuracy: 0.3369 - loss: 3.0812\n","Epoch 30/50\n","\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 215ms/step - accuracy: 0.3681 - loss: 2.9904\n","Epoch 31/50\n","\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 196ms/step - accuracy: 0.3866 - loss: 2.8640\n","Epoch 32/50\n","\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 269ms/step - accuracy: 0.3713 - loss: 2.8299\n","Epoch 33/50\n","\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 214ms/step - accuracy: 0.3548 - loss: 2.8403\n","Epoch 34/50\n","\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 224ms/step - accuracy: 0.3892 - loss: 2.6886\n","Epoch 35/50\n","\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 215ms/step - accuracy: 0.3827 - loss: 2.6793\n","Epoch 36/50\n","\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 288ms/step - accuracy: 0.3772 - loss: 2.7013\n","Epoch 37/50\n","\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 196ms/step - accuracy: 0.4035 - loss: 2.5661\n","Epoch 38/50\n","\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 242ms/step - accuracy: 0.4426 - loss: 2.4635\n","Epoch 39/50\n","\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 182ms/step - accuracy: 0.4330 - loss: 2.4482\n","Epoch 40/50\n","\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 256ms/step - accuracy: 0.4329 - loss: 2.4180\n","Epoch 41/50\n","\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 187ms/step - accuracy: 0.4567 - loss: 2.2810\n","Epoch 42/50\n","\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 256ms/step - accuracy: 0.4670 - loss: 2.2863\n","Epoch 43/50\n","\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 205ms/step - accuracy: 0.4750 - loss: 2.2754\n","Epoch 44/50\n","\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 249ms/step - accuracy: 0.4842 - loss: 2.1772\n","Epoch 45/50\n","\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 224ms/step - accuracy: 0.4872 - loss: 2.1684\n","Epoch 46/50\n","\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 186ms/step - accuracy: 0.5387 - loss: 1.9853\n","Epoch 47/50\n","\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 253ms/step - accuracy: 0.5305 - loss: 2.0127\n","Epoch 48/50\n","\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 233ms/step - accuracy: 0.5140 - loss: 2.0163\n","Epoch 49/50\n","\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 203ms/step - accuracy: 0.5369 - loss: 1.9692\n","Epoch 50/50\n","\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 258ms/step - accuracy: 0.5480 - loss: 1.8928\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.history.History at 0x78104e559000>"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["# Next Word Prediction"],"metadata":{"id":"M2VKqWWHj88i"}},{"cell_type":"code","source":["# Function to predict the next word(s) given a seed text\n","def predict_next_word(seed_text, next_words=1):\n","    for _ in range(next_words):\n","        # Convert the seed text to a sequence of integers\n","        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n","\n","        # Pad the sequence to match the input length required by the model\n","        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n","\n","        # Predict the probabilities of the next word in the sequence\n","        predicted = model.predict(token_list, verbose=0)\n","\n","        # Get the index of the word with the highest probability\n","        predicted_word_index = np.argmax(predicted, axis=-1)[0]\n","\n","        # Retrieve the word corresponding to the predicted index\n","        predicted_word = tokenizer.index_word[predicted_word_index]\n","\n","        # Append the predicted word to the seed text\n","        seed_text += \" \" + predicted_word\n","\n","    # Return the updated seed text with the predicted word(s)\n","    return seed_text\n"],"metadata":{"id":"uA7fOg_Lj8N8","executionInfo":{"status":"ok","timestamp":1724366187043,"user_tz":-540,"elapsed":412,"user":{"displayName":"Rao Hammad Raza","userId":"12244803790304730718"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["# Test the Prediction function with a sample input"],"metadata":{"id":"JK1wHWnGlfvr"}},{"cell_type":"code","source":["print(predict_next_word(\"I love the\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GiVb1vqFlnPJ","executionInfo":{"status":"ok","timestamp":1724366191747,"user_tz":-540,"elapsed":1191,"user":{"displayName":"Rao Hammad Raza","userId":"12244803790304730718"}},"outputId":"84640701-6572-4527-e928-ce5017ad45b6"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["I love the ethical\n"]}]},{"cell_type":"markdown","source":["# Building a Long Short-Term Memory (LSTM)"],"metadata":{"id":"TRniGOVqShGv"}},{"cell_type":"code","source":["# Import necessary libraries\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, LSTM, Embedding\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences"],"metadata":{"id":"MH8Pcs3ySd_e","executionInfo":{"status":"ok","timestamp":1724366200157,"user_tz":-540,"elapsed":391,"user":{"displayName":"Rao Hammad Raza","userId":"12244803790304730718"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["# Sample Data"],"metadata":{"id":"m334kZCtTrow"}},{"cell_type":"code","source":["\n","# Sample data\n","sentences = [\n","    \"Machine learning algorithms are powerful tools.\"\n","    \"I enjoy exploring new algorithms.\"\n","    \"Learning about AI is captivating.\"\n","    \"Deep learning models can be complex.\"\n","    \"I love understanding how neural networks work.\"\n","    \"The field of data science is evolving rapidly.\"\n","    \"I find artificial intelligence intriguing.\"\n","    \"Studying computer vision is exciting.\"\n","    \"Natural language processing is a fascinating domain.\"\n","    \"I enjoy coding in Python for data science projects.\"\n","    \"Building models is a creative process.\"\n","    \"I love experimenting with different machine learning techniques.\"\n","    \"The potential of AI to transform industries is amazing.\"\n","    \"I enjoy staying updated with the latest tech trends.\"\n","    \"Learning about reinforcement learning is interesting.\"\n","    \"I find predictive modeling to be very useful.\"\n","    \"I love solving problems with data analysis.\"\n","    \"Data preprocessing is a crucial step in machine learning.\"\n","    \"I enjoy reading research papers on deep learning.\"\n","    \"I find optimization techniques fascinating.\"\n","    \"Understanding algorithms helps in developing better solutions.\"\n","    \"I love the challenge of debugging code.\"\n","    \"Machine learning applications are diverse and impactful.\"\n","    \"I enjoy collaborating with others on tech projects.\"\n","    \"Learning new programming languages is fun.\"\n","    \"I love working with large datasets.\"\n","    \"I find feature engineering to be an art.\"\n","    \"Model evaluation is an essential part of machine learning.\"\n","    \"I enjoy attending tech conferences.\"\n","    \"Learning about big data technologies is exciting.\"\n","    \"I love experimenting with neural network architectures.\"\n","    \"I find the theory behind machine learning algorithms interesting.\"\n","    \"I enjoy visualizing data insights.\"\n","    \"Machine learning models can make accurate predictions.\"\n","    \"I love the creativity involved in data storytelling.\"\n","    \"I find unsupervised learning techniques intriguing.\"\n","    \"I enjoy automating tasks with AI.\"\n","    \"Learning about AI ethics is important.\"\n","    \"I love the problem-solving aspect of machine learning.\"\n","    \"I find cloud computing technologies fascinating.\"\n","    \"I enjoy using machine learning for real-world applications.\"\n","    \"I love experimenting with different data preprocessing techniques.\"\n","    \"I find transfer learning to be a powerful approach.\"\n","    \"I enjoy working on machine learning projects.\"\n","    \"Learning about data privacy is crucial.\"\n","    \"I love the innovation happening in the AI field.\"\n","    \"I find data visualization tools useful.\"\n","    \"I enjoy testing and validating machine learning models.\"\n","    \"I love discovering new machine learning applications.\"\n","    \"I find ensemble methods to be effective.\"\n","    \"I enjoy learning from data.\"\n","    \"Machine learning can provide valuable insights.\"\n","    \"I love the interdisciplinary nature of AI.\"\n","    \"I find recommendation systems interesting.\"\n","    \"I enjoy participating in hackathons.\"\n","    \"Learning about neural networks is fascinating.\"\n","    \"I love the potential of AI to solve complex problems.\"\n","    \"I find sentiment analysis intriguing.\"\n","    \"I enjoy implementing machine learning algorithms.\"\n","    \"I love the excitement of discovering patterns in data.\"\n","    \"I find time series analysis challenging.\"\n","    \"I enjoy exploring different types of data.\"\n","    \"Machine learning is transforming various industries.\"\n","    \"I love working on predictive analytics.\"\n","    \"I find anomaly detection to be useful.\"\n","    \"I enjoy studying the mathematics behind machine learning.\"\n","    \"I love the hands-on experience of building models.\"\n","    \"I find clustering techniques interesting.\"\n","    \"I enjoy exploring open-source machine learning libraries.\"\n","    \"Machine learning can automate complex tasks.\"\n","    \"I love the flexibility of machine learning models.\"\n","    \"I find computer vision applications fascinating.\"\n","    \"I enjoy solving real-world problems with AI.\"\n","    \"I love the continuous learning aspect of AI.\"\n","    \"I find reinforcement learning to be challenging.\"\n","    \"I enjoy experimenting with hyperparameter tuning.\"\n","    \"Machine learning can improve decision-making processes.\"\n","    \"I love the creativity involved in feature selection.\"\n","    \"I find generative models to be fascinating.\"\n","    \"I enjoy reading about the latest AI advancements.\"\n","    \"Machine learning can enhance user experiences.\"\n","    \"I love the diversity of machine learning applications.\"\n","    \"I find natural language generation intriguing.\"\n","    \"I enjoy working with text data.\"\n","    \"Machine learning can optimize business processes.\"\n","    \"I love the innovation in AI research.\"\n","    \"I find the concept of machine learning interpretability interesting.\"\n","    \"I enjoy creating machine learning workflows.\"\n","    \"Machine learning can uncover hidden patterns.\"\n","    \"I love the impact of AI on society.\"\n","    \"I find deep reinforcement learning fascinating.\"\n","    \"I enjoy developing custom machine learning solutions.\"\n","    \"Machine learning can improve customer experiences.\"\n","    \"I love the potential of AI in healthcare.\"\n","    \"I find the scalability of machine learning models intriguing.\"\n","    \"I enjoy applying machine learning to finance.\"\n","    \"Machine learning can enhance security measures.\"\n","    \"I love the possibilities of AI in creative industries.\"\n","    \"I find the ethical implications of AI important.\"\n","    \"I enjoy sharing knowledge about machine learning.\"\n","]"],"metadata":{"id":"KUzEYS4HTuh1","executionInfo":{"status":"ok","timestamp":1724366203163,"user_tz":-540,"elapsed":382,"user":{"displayName":"Rao Hammad Raza","userId":"12244803790304730718"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["# Tokenization and Preprocessing of Data"],"metadata":{"id":"faA4QXdDTyel"}},{"cell_type":"code","source":["tokenizer = Tokenizer()\n","\n","tokenizer.fit_on_texts(sentences)\n","\n","total_words = len(tokenizer.word_index) + 1\n","\n","input_sequences = []\n","\n","for line in sentences:\n","  token_list  = tokenizer.texts_to_sequences([line])[0]\n","\n","  for  i in range(1, len(token_list)):\n","    n_gram_sequence = token_list[:i + 1]\n","    input_sequences.append(n_gram_sequence)\n","\n","mak_sequence_len = max([len(x) for x in input_sequences])\n","\n","input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))"],"metadata":{"id":"p0trbB0pUB13","executionInfo":{"status":"ok","timestamp":1724366208803,"user_tz":-540,"elapsed":402,"user":{"displayName":"Rao Hammad Raza","userId":"12244803790304730718"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["X = input_sequences[:, :-1]\n","\n","y = input_sequences[:, -1]\n","\n","y = tf.keras.utils.to_categorical(y, num_classes=total_words)"],"metadata":{"id":"qWvDWOBdVTm_","executionInfo":{"status":"ok","timestamp":1724366221801,"user_tz":-540,"elapsed":415,"user":{"displayName":"Rao Hammad Raza","userId":"12244803790304730718"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["# Defining the LSTM Model"],"metadata":{"id":"sGXk6rDyVppz"}},{"cell_type":"code","source":["# Define the LSTM model\n","\n","# Sequential model allows stacking layers in a linear fashion\n","model = Sequential([\n","    # Embedding layer: Converts word indices to dense vectors\n","    # Input dimension: total number of words, Output dimension: size of embedding vectors\n","    # Input length: length of input sequences (excluding the last word)\n","    Embedding(total_words, 10, input_length=max_sequence_len-1),\n","\n","    # LSTM layer with 30 units\n","    # LSTM (Long Short-Term Memory) is a type of RNN that can capture long-term dependencies\n","    LSTM(100),\n","\n","    # Dense output layer with a softmax activation function\n","    # Output dimension: total number of words (for multi-class classification)\n","    # Softmax activation converts the output to probabilities for each word\n","    Dense(total_words, activation='softmax')\n","])\n"],"metadata":{"id":"-q4f67mhVyPE","executionInfo":{"status":"ok","timestamp":1724366224755,"user_tz":-540,"elapsed":5,"user":{"displayName":"Rao Hammad Raza","userId":"12244803790304730718"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["# Compiling the model"],"metadata":{"id":"roJRkLVuWAQ-"}},{"cell_type":"code","source":["\n","# Compile the model\n","\n","# Compile the model specifies the loss function, optimizer, and evaluation metrics\n","model.compile(\n","    # Loss function: Categorical cross-entropy, used for multi-class classification problems\n","    loss='categorical_crossentropy',\n","\n","    # Optimizer: Adam, an efficient optimization algorithm that adjusts the learning rate during training\n","    optimizer='adam',\n","\n","    # Metrics: Accuracy, to evaluate the model's performance during training and testing\n","    metrics=['accuracy']\n",")\n","\n"],"metadata":{"id":"OKIgxGF_V_GI","executionInfo":{"status":"ok","timestamp":1724366239302,"user_tz":-540,"elapsed":400,"user":{"displayName":"Rao Hammad Raza","userId":"12244803790304730718"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["# Training the Model"],"metadata":{"id":"hIYtb-duWJQc"}},{"cell_type":"code","source":["\n","# Train the model\n","\n","# Fit the model on the training data\n","# X: Input sequences\n","# y: One-hot encoded labels\n","# epochs: Number of times the model will go through the entire dataset\n","# verbose: Controls the verbosity of the output during training (1 for progress bar)\n","model.fit(X, y, epochs=50, verbose=1)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TtLAEeMcWMNg","executionInfo":{"status":"ok","timestamp":1724367083857,"user_tz":-540,"elapsed":841966,"user":{"displayName":"Rao Hammad Raza","userId":"12244803790304730718"}},"outputId":"a7cf05bb-31a7-4bad-9770-aa02df469b4e"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 524ms/step - accuracy: 0.0645 - loss: 5.3658\n","Epoch 2/50\n","\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 545ms/step - accuracy: 0.1177 - loss: 4.6673\n","Epoch 3/50\n","\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 546ms/step - accuracy: 0.0941 - loss: 4.5564\n","Epoch 4/50\n","\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 488ms/step - accuracy: 0.1013 - loss: 4.5590\n","Epoch 5/50\n","\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 489ms/step - accuracy: 0.0936 - loss: 4.4557\n","Epoch 6/50\n","\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 530ms/step - accuracy: 0.1017 - loss: 4.5228\n","Epoch 7/50\n","\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 511ms/step - accuracy: 0.1205 - loss: 4.4039\n","Epoch 8/50\n","\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 461ms/step - accuracy: 0.1094 - loss: 4.5016\n","Epoch 9/50\n","\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 526ms/step - accuracy: 0.0987 - loss: 4.4956\n","Epoch 10/50\n","\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 538ms/step - accuracy: 0.1166 - loss: 4.4680\n","Epoch 11/50\n","\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 489ms/step - accuracy: 0.1688 - loss: 4.3419\n","Epoch 12/50\n","\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 529ms/step - accuracy: 0.2060 - loss: 4.0964\n","Epoch 13/50\n","\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 546ms/step - accuracy: 0.1963 - loss: 4.1501\n","Epoch 14/50\n","\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 510ms/step - accuracy: 0.2447 - loss: 4.0455\n","Epoch 15/50\n","\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 527ms/step - accuracy: 0.2371 - loss: 3.9892\n","Epoch 16/50\n","\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 516ms/step - accuracy: 0.2138 - loss: 3.9701\n","Epoch 17/50\n","\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 453ms/step - accuracy: 0.2420 - loss: 3.8483\n","Epoch 18/50\n","\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 486ms/step - accuracy: 0.2654 - loss: 3.6414\n","Epoch 19/50\n","\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 650ms/step - accuracy: 0.2332 - loss: 3.7615\n","Epoch 20/50\n","\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 521ms/step - accuracy: 0.2703 - loss: 3.5203\n","Epoch 21/50\n","\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 485ms/step - accuracy: 0.2816 - loss: 3.5334\n","Epoch 22/50\n","\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 516ms/step - accuracy: 0.2810 - loss: 3.4233\n","Epoch 23/50\n","\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 517ms/step - accuracy: 0.3230 - loss: 3.3415\n","Epoch 24/50\n","\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 493ms/step - accuracy: 0.3223 - loss: 3.2339\n","Epoch 25/50\n","\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 519ms/step - accuracy: 0.3310 - loss: 3.1004\n","Epoch 26/50\n","\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 518ms/step - accuracy: 0.3387 - loss: 3.1330\n","Epoch 27/50\n","\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 487ms/step - accuracy: 0.3691 - loss: 3.0128\n","Epoch 28/50\n","\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 528ms/step - accuracy: 0.3416 - loss: 3.0583\n","Epoch 29/50\n","\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 471ms/step - accuracy: 0.3901 - loss: 2.8901\n","Epoch 30/50\n","\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 519ms/step - accuracy: 0.3640 - loss: 2.9471\n","Epoch 31/50\n","\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 517ms/step - accuracy: 0.3699 - loss: 2.8726\n","Epoch 32/50\n","\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 492ms/step - accuracy: 0.3780 - loss: 2.7642\n","Epoch 33/50\n","\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 527ms/step - accuracy: 0.3926 - loss: 2.6837\n","Epoch 34/50\n","\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 545ms/step - accuracy: 0.3822 - loss: 2.6952\n","Epoch 35/50\n","\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 501ms/step - accuracy: 0.4293 - loss: 2.5354\n","Epoch 36/50\n","\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 516ms/step - accuracy: 0.4230 - loss: 2.5847\n","Epoch 37/50\n","\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 453ms/step - accuracy: 0.4272 - loss: 2.5722\n","Epoch 38/50\n","\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 482ms/step - accuracy: 0.4256 - loss: 2.4865\n","Epoch 39/50\n","\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 518ms/step - accuracy: 0.4155 - loss: 2.4110\n","Epoch 40/50\n","\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 452ms/step - accuracy: 0.4440 - loss: 2.3745\n","Epoch 41/50\n","\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 481ms/step - accuracy: 0.4696 - loss: 2.2541\n","Epoch 42/50\n","\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 519ms/step - accuracy: 0.4700 - loss: 2.2823\n","Epoch 43/50\n","\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 522ms/step - accuracy: 0.4648 - loss: 2.1990\n","Epoch 44/50\n","\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 511ms/step - accuracy: 0.4932 - loss: 2.1556\n","Epoch 45/50\n","\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 523ms/step - accuracy: 0.5019 - loss: 2.0407\n","Epoch 46/50\n","\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 509ms/step - accuracy: 0.5052 - loss: 2.0319\n","Epoch 47/50\n","\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 443ms/step - accuracy: 0.4966 - loss: 2.0555\n","Epoch 48/50\n","\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 515ms/step - accuracy: 0.5002 - loss: 2.0015\n","Epoch 49/50\n","\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 520ms/step - accuracy: 0.5184 - loss: 1.8948\n","Epoch 50/50\n","\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 510ms/step - accuracy: 0.5241 - loss: 1.8825\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.history.History at 0x7975dbd30b20>"]},"metadata":{},"execution_count":17}]},{"cell_type":"markdown","source":["# Prediction of Next Word"],"metadata":{"id":"S-H9TQEXWTcu"}},{"cell_type":"code","source":["# Function to predict the next word(s) given a seed text\n","def predict_next_word(seed_text, next_words=2):\n","    # Repeat the prediction process for the specified number of next words\n","    for _ in range(next_words):\n","        # Convert the seed text into a sequence of integers using the tokenizer\n","        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n","\n","        # Pad the sequence to ensure it matches the input length expected by the model\n","        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n","\n","        # Predict the probabilities of the next word in the sequence\n","        predicted = model.predict(token_list, verbose=0)\n","\n","        # Get the index of the word with the highest probability\n","        predicted_word_index = np.argmax(predicted, axis=-1)[0]\n","\n","        # Retrieve the word corresponding to the predicted index\n","        predicted_word = tokenizer.index_word[predicted_word_index]\n","\n","        # Append the predicted word to the seed text\n","        seed_text += \" \" + predicted_word\n","\n","    # Return the updated seed text with the predicted word(s)\n","    return seed_text\n"],"metadata":{"id":"wxqUTpq-WYiz","executionInfo":{"status":"ok","timestamp":1724367090935,"user_tz":-540,"elapsed":386,"user":{"displayName":"Rao Hammad Raza","userId":"12244803790304730718"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["# Test prediction\n","print(predict_next_word(\"Machine Learning can improve\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZpDknjf2WcS5","executionInfo":{"status":"ok","timestamp":1724367095536,"user_tz":-540,"elapsed":1404,"user":{"displayName":"Rao Hammad Raza","userId":"12244803790304730718"}},"outputId":"af33a666-7fa3-4063-dff8-f0920808f7b3"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Machine Learning can improve fascinating i\n"]}]}]}